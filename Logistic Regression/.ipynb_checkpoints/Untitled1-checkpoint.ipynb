{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    # Todo: encode categorical features using OneHotEncoder class\n",
    "    # Todo: standardize numerical features using StandardScaler()class\n",
    "    # Todo: return processed training and test sets\n",
    "    train_processed = pd.DataFrame(data = {\"age\": StandardScaler().fit_transform(X_train.iloc[:, 0:1]).T[0]})\n",
    "    test_processed = pd.DataFrame(data = {\"age\": StandardScaler().fit_transform(X_test.iloc[:, 0:1]).T[0]})\n",
    "    col = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "    ohe = OneHotEncoder()\n",
    "    for i in col:\n",
    "        train_col = np.unique(X_train[i].values)\n",
    "        test_col = np.unique(X_test[i].values)\n",
    "        train = pd.DataFrame(data=ohe.fit_transform(X_train[i].values.reshape(-1, 1)).toarray(), columns=train_col)\n",
    "        test = pd.DataFrame(data=ohe.fit_transform(X_test[i].values.reshape(-1, 1)).toarray(), columns=test_col)\n",
    "        \n",
    "        for f in train_col:\n",
    "            title = i + \".\" + f\n",
    "            train_processed[title] = train[f]\n",
    "            \n",
    "        for t in test_col:\n",
    "            title = i + \".\" + t\n",
    "            test_processed[title] = test[t]\n",
    "            \n",
    "    stnd_col = [\"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "    stnd_train = StandardScaler().fit_transform(X_train.iloc[:, 8:11]).T \n",
    "    stnd_test = StandardScaler().fit_transform(X_test.iloc[:, 8:11]).T\n",
    "    for z in range(3):\n",
    "        title = \"standardized.\" + stnd_col[z]\n",
    "        train_processed[title] = stnd_train[z]\n",
    "        test_processed[title] = stnd_test[z]\n",
    "    \n",
    "    # might have to add columns of zeros for test data that doesnt contain all outcomes as train\n",
    "    # columns would be {'native.country.Holand-Netherlands'(col 63),'workclass.Never-worked'(col 3),'workclass.Without-pay'(col 8)}\n",
    "    test_processed.insert(3, \"workclass.Never-worked\", np.zeros((len(test_processed))))\n",
    "    test_processed.insert(8, \"workclass.Without-pay\", np.zeros((len(test_processed))))\n",
    "    test_processed.insert(63, \"native.country.Holand-Netherlands\", np.zeros((len(test_processed))))\n",
    "    return train_processed, test_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000):\n",
    "        '''Initialize variables\n",
    "        Args:\n",
    "            learning_rate  : Learning Rate\n",
    "            max_iterations : Max iterations for training weights\n",
    "        '''\n",
    "        # Initialising all the parameters\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.likelihoods    = []\n",
    "        \n",
    "        # Define epsilon because log(0) is not defined\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        '''Sigmoid function: f:R->(0,1)\n",
    "        Args:\n",
    "            z : A numpy array (num_samples,)\n",
    "        Returns:\n",
    "            A numpy array where sigmoid function applied to every element\n",
    "        '''\n",
    "        ### START CODE HERE\n",
    "        sig_z = (1/(1+np.exp(-z)))\n",
    "        ### END CODE HERE\n",
    "        \n",
    "        assert (z.shape==sig_z.shape), 'Error in sigmoid implementation. Check carefully'\n",
    "        return sig_z\n",
    "    \n",
    "    def log_likelihood(self, y_true, y_pred):\n",
    "        '''Calculates maximum likelihood estimate\n",
    "        Remember: y * log(yh) + (1-y) * log(1-yh)\n",
    "        Note: Likelihood is defined for multiple classes as well, but for this dataset\n",
    "        we only need to worry about binary/bernoulli likelihood function\n",
    "        Args:\n",
    "                    y_true : Numpy array of actual truth values (num_samples,)\n",
    "            y_pred : Numpy array of predicted values (num_samples,)\n",
    "        Returns:\n",
    "            Log-likelihood, scalar value\n",
    "        '''\n",
    "        # Fix 0/1 values in y_pred so that log is not undefined\n",
    "        y_pred = np.maximum(np.full(y_pred.shape, self.eps), np.minimum(np.full(y_pred.shape, 1-self.eps), y_pred))\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        likelihood = (y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))\n",
    "    \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return np.mean(likelihood)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''Trains logistic regression model using gradient ascent\n",
    "        to gain maximum likelihood on the training data\n",
    "        Args:\n",
    "            X : Numpy array (num_examples, num_features)\n",
    "            y : Numpy array (num_examples, )\n",
    "        Returns: VOID\n",
    "        '''\n",
    "        \n",
    "        num_examples = X.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        ### START CODE HERE\n",
    "        \n",
    "        # Initialize weights with appropriate shape\n",
    "        self.weights = np.zeros((X.shape[1]))\n",
    "        # print(\"Z\",self.weights.shape)\n",
    "        # print(X.shape)\n",
    "               \n",
    "        \n",
    "        # Perform gradient ascent\n",
    "        for i in range(self.max_iterations):\n",
    "            # Define the linear hypothesis(z) first\n",
    "            # HINT: what is our hypothesis function in linear regression, remember?\n",
    "            \n",
    "            z  = np.dot(X,self.weights)\n",
    "          \n",
    "            # Output probability value by appplying sigmoid on z\n",
    "            y_pred = self.sigmoid(z)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Calculate the gradient values\n",
    "            # This is just vectorized efficient way of implementing gradient. Don't worry, we will discuss it later.\n",
    "            gradient = np.mean((y-y_pred)*X.T, axis=1)\n",
    "            \n",
    "            # Update the weights\n",
    "            # Caution: It is gradient ASCENT not descent\n",
    "            self.weights +=  self.learning_rate*gradient\n",
    "            \n",
    "            # Calculating log likelihood\n",
    "            likelihood = self.log_likelihood(y,y_pred)\n",
    "\n",
    "            self.likelihoods.append(likelihood)\n",
    "    \n",
    "        ### END CODE HERE\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        '''Predict probabilities for given X.\n",
    "        Remember sigmoid returns value between 0 and 1.\n",
    "        Args:\n",
    "                   X : Numpy array (num_samples, num_features)\n",
    "        Returns:\n",
    "            probabilities: Numpy array (num_samples,)\n",
    "        '''\n",
    "        if self.weights is None:\n",
    "            raise Exception(\"Fit the model before prediction\")\n",
    "      \n",
    "        ### START CODE HERE\n",
    "               \n",
    "        z = np.dot(X,self.weights)\n",
    "        probabilities = self.sigmoid(z)\n",
    "        # probabilities.reshape(probabilities.shape[0],1)\n",
    "        \n",
    "        ### END CODE HERE\n",
    "        \n",
    "        return probabilities\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        '''Predict/Classify X in classes\n",
    "        Args:\n",
    "            X         : Numpy array (num_samples, num_features)\n",
    "            threshold : scalar value above which prediction is 1 else 0\n",
    "        Returns:\n",
    "            binary_predictions : Numpy array (num_samples,)\n",
    "        '''\n",
    "        # Thresholding probability to predict binary values\n",
    "        \n",
    "        binary_predictions = np.array(list(map(lambda x: 1 if x>threshold else 0, self.predict_proba(X))))\n",
    "        \n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cleaned_train_data.csv')\n",
    "test_data = pd.read_csv('cleaned_test_data.csv')\n",
    "X_train = train_data.drop(['income'], axis=1, inplace=False)\n",
    "y_train = train_data['income']\n",
    "X_test = test_data.drop(['income'], axis=1, inplace=False)\n",
    "y_test = test_data['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed, test_processed = preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((train_processed.shape[0], 1)), train_processed))\n",
    "y = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLogisticRegression(learning_rate=0.75, max_iterations=250)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.29364656e-01,  2.98929989e-01,  2.46610047e-01, -1.47097409e-01,\n",
       "       -3.14363938e-03, -2.10291716e-01,  8.81068606e-02, -5.54069124e-01,\n",
       "       -1.29215468e-01, -2.02642068e-02, -6.18641721e-02, -1.14673045e-01,\n",
       "        4.06122571e-01,  4.09199646e-01, -7.11330627e-01,  5.66435098e-01,\n",
       "        5.12257622e-01, -1.37072111e+00, -3.64790636e-01, -3.45115641e-01,\n",
       "        1.69789053e-02,  9.61287795e-01, -9.95405174e-02, -8.71998133e-01,\n",
       "       -1.89706878e-01, -2.01270188e-01, -1.16700518e-01, -2.55957705e-03,\n",
       "       -4.24527488e-02,  7.27357144e-01, -5.99665621e-01, -3.90533134e-01,\n",
       "       -3.22289869e-01, -6.33616193e-01, -6.03310740e-02,  3.47251613e-01,\n",
       "        1.25292188e-01,  1.63803220e-01,  2.86488153e-01, -2.11408239e-01,\n",
       "        2.21095024e-01, -2.75653638e-01, -2.60033050e-01, -7.30305013e-01,\n",
       "       -4.88120523e-01,  8.03652544e-01, -4.99326296e-01, -2.30038360e-01,\n",
       "       -5.83149122e-01, -1.46215534e-01,  1.02777766e-02, -7.54098508e-05,\n",
       "       -2.06652899e-02, -4.21245801e-02,  7.90014127e-03, -2.88851629e-02,\n",
       "       -8.87331520e-03, -4.43428325e-02,  5.97799503e-03,  1.13388478e-02,\n",
       "        2.36560608e-02, -2.69181777e-02, -1.49489500e-02, -1.11620442e-02,\n",
       "       -1.01536874e-03, -4.01059781e-03, -5.27222036e-03, -4.19650033e-03,\n",
       "       -2.13135764e-02,  1.84367550e-04,  4.63569005e-03,  2.37409951e-02,\n",
       "       -1.48636609e-02,  1.73555007e-02, -1.00684041e-02, -2.88419472e-01,\n",
       "       -2.00592444e-02, -7.44813667e-03, -1.35105721e-02,  3.85711739e-02,\n",
       "       -2.78015376e-02, -2.80280302e-02, -4.55056937e-02,  1.66386362e-03,\n",
       "       -5.02233930e-02,  5.84065914e-03, -3.43474427e-03, -1.10395215e-02,\n",
       "       -7.95491854e-02, -4.42249102e-02, -2.52719586e-03,  7.89993544e-01,\n",
       "        2.64496629e-01,  3.86226864e-01])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.6931471805599453,\n",
       " -0.5182731876053012,\n",
       " -0.4796228585645618,\n",
       " -0.4556134912768147,\n",
       " -0.43886383278695484,\n",
       " -0.42653690352731677,\n",
       " -0.41708838431260487,\n",
       " -0.409603459022619,\n",
       " -0.4035114884154514,\n",
       " -0.3984407741512817,\n",
       " -0.3941400249277944,\n",
       " -0.39043384210621335,\n",
       " -0.3871963891116437,\n",
       " -0.38433521058125947,\n",
       " -0.381780947513023,\n",
       " -0.37948060277941487,\n",
       " -0.3773930141895813,\n",
       " -0.37548573988013006,\n",
       " -0.37373287055140775,\n",
       " -0.3721134639542405,\n",
       " -0.3706104057699039,\n",
       " -0.3692095681224713,\n",
       " -0.3678991793562068,\n",
       " -0.3666693460786654,\n",
       " -0.3655116864882112,\n",
       " -0.36441904608193243,\n",
       " -0.36338527506971574,\n",
       " -0.36240505251405686,\n",
       " -0.3614737462101086,\n",
       " -0.3605873001601037,\n",
       " -0.359742143539213,\n",
       " -0.3589351165363182,\n",
       " -0.3581634095460259,\n",
       " -0.3574245129997418,\n",
       " -0.35671617573167586,\n",
       " -0.3560363702352787,\n",
       " -0.3553832635158741,\n",
       " -0.3547551925141357,\n",
       " -0.3541506432831256,\n",
       " -0.3535682332635244,\n",
       " -0.3530066961286633,\n",
       " -0.35246486877105937,\n",
       " -0.3519416800815186,\n",
       " -0.3514361412352291,\n",
       " -0.35094733724997074,\n",
       " -0.350474419622478,\n",
       " -0.35001659988208195,\n",
       " -0.34957314392769817,\n",
       " -0.3491433670361356,\n",
       " -0.3487266294478036,\n",
       " -0.34832233245069255,\n",
       " -0.34792991489574754,\n",
       " -0.34754885008697517,\n",
       " -0.3471786429980555,\n",
       " -0.34681882777430195,\n",
       " -0.34646896548476075,\n",
       " -0.3461286420942171,\n",
       " -0.34579746662903865,\n",
       " -0.3454750695144212,\n",
       " -0.34516110106351466,\n",
       " -0.3448552301015632,\n",
       " -0.3445571427103054,\n",
       " -0.3442665410798292,\n",
       " -0.3439831424565799,\n",
       " -0.3437066781776974,\n",
       " -0.3434368927830048,\n",
       " -0.34317354319696336,\n",
       " -0.34291639797385387,\n",
       " -0.34266523660021936,\n",
       " -0.3424198488491873,\n",
       " -0.3421800341820175,\n",
       " -0.34194560119259226,\n",
       " -0.34171636709110786,\n",
       " -0.3414921572235993,\n",
       " -0.341272804624247,\n",
       " -0.3410581495977503,\n",
       " -0.34084803932932145,\n",
       " -0.3406423275200659,\n",
       " -0.34044087404578605,\n",
       " -0.3402435446373353,\n",
       " -0.34005021058093027,\n",
       " -0.3398607484368983,\n",
       " -0.3396750397754943,\n",
       " -0.3394929709285759,\n",
       " -0.3393144327559619,\n",
       " -0.3391393204254443,\n",
       " -0.33896753320555656,\n",
       " -0.3387989742701205,\n",
       " -0.3386335505138976,\n",
       " -0.3384711723784859,\n",
       " -0.33831175368787947,\n",
       " -0.3381552114930016,\n",
       " -0.3380014659246713,\n",
       " -0.3378504400544572,\n",
       " -0.3377020597628916,\n",
       " -0.3375562536146657,\n",
       " -0.33741295274027794,\n",
       " -0.33727209072384245,\n",
       " -0.3371336034966135,\n",
       " -0.33699742923591286,\n",
       " -0.3368635082691803,\n",
       " -0.3367317829827607,\n",
       " -0.33660219773525574,\n",
       " -0.336474698775096,\n",
       " -0.33634923416216755,\n",
       " -0.33622575369318064,\n",
       " -0.3361042088306752,\n",
       " -0.33598455263537197,\n",
       " -0.33586673970172914,\n",
       " -0.3357507260965384,\n",
       " -0.3356364693003808,\n",
       " -0.3355239281517793,\n",
       " -0.33541306279394417,\n",
       " -0.33530383462392416,\n",
       " -0.33519620624409097,\n",
       " -0.3350901414158012,\n",
       " -0.3349856050151093,\n",
       " -0.3348825629905084,\n",
       " -0.334780982322475,\n",
       " -0.3346808309848381,\n",
       " -0.33458207790781413,\n",
       " -0.3344846929426428,\n",
       " -0.33438864682777997,\n",
       " -0.33429391115648277,\n",
       " -0.33420045834584033,\n",
       " -0.3341082616070624,\n",
       " -0.3340172949170668,\n",
       " -0.3339275329912004,\n",
       " -0.3338389512571668,\n",
       " -0.3337515258299589,\n",
       " -0.33366523348788185,\n",
       " -0.333580051649523,\n",
       " -0.3334959583516668,\n",
       " -0.3334129322281169,\n",
       " -0.33333095248935246,\n",
       " -0.33324999890300677,\n",
       " -0.3331700517751114,\n",
       " -0.33309109193211456,\n",
       " -0.33301310070356044,\n",
       " -0.3329360599054921,\n",
       " -0.33285995182445777,\n",
       " -0.3327847592021737,\n",
       " -0.3327104652207633,\n",
       " -0.33263705348854355,\n",
       " -0.3325645080263877,\n",
       " -0.3324928132545709,\n",
       " -0.3324219539801452,\n",
       " -0.33235191538475034,\n",
       " -0.3322826830129089,\n",
       " -0.33221424276075406,\n",
       " -0.33214658086514515,\n",
       " -0.33207968389321785,\n",
       " -0.3320135387322969,\n",
       " -0.33194813258017447,\n",
       " -0.3318834529357523,\n",
       " -0.3318194875900056,\n",
       " -0.3317562246172769,\n",
       " -0.3316936523668887,\n",
       " -0.3316317594550316,\n",
       " -0.3315705347569638,\n",
       " -0.3315099673994544,\n",
       " -0.3314500467535189,\n",
       " -0.33139076242738585,\n",
       " -0.3313321042597282,\n",
       " -0.3312740623130886,\n",
       " -0.33121662686758313,\n",
       " -0.3311597884147569,\n",
       " -0.33110353765170275,\n",
       " -0.33104786547535575,\n",
       " -0.33099276297694935,\n",
       " -0.3309382214367237,\n",
       " -0.33088423231873376,\n",
       " -0.3308307872658827,\n",
       " -0.33077787809510184,\n",
       " -0.33072549679267116,\n",
       " -0.33067363550971623,\n",
       " -0.3306222865578302,\n",
       " -0.3305714424048636,\n",
       " -0.3305210956708136,\n",
       " -0.33047123912386206,\n",
       " -0.3304218656765496,\n",
       " -0.33037296838205366,\n",
       " -0.3303245404305797,\n",
       " -0.3302765751458914,\n",
       " -0.33022906598190244,\n",
       " -0.33018200651942914,\n",
       " -0.33013539046300217,\n",
       " -0.33008921163777905,\n",
       " -0.3300434639865731,\n",
       " -0.329998141566956,\n",
       " -0.3299532385484428,\n",
       " -0.3299087492097743,\n",
       " -0.32986466793626806,\n",
       " -0.329820989217264,\n",
       " -0.32977770764362335,\n",
       " -0.329734817905323,\n",
       " -0.3296923147890949,\n",
       " -0.3296501931761656,\n",
       " -0.3296084480400421,\n",
       " -0.3295670744443432,\n",
       " -0.32952606754074293,\n",
       " -0.32948542256691754,\n",
       " -0.3294451348445866,\n",
       " -0.3294051997775995,\n",
       " -0.3293656128500776,\n",
       " -0.32932636962458245,\n",
       " -0.3292874657403929,\n",
       " -0.3292488969117627,\n",
       " -0.3292106589262705,\n",
       " -0.3291727476432129,\n",
       " -0.3291351589919995,\n",
       " -0.3290978889706583,\n",
       " -0.3290609336443174,\n",
       " -0.3290242891437654,\n",
       " -0.3289879516640458,\n",
       " -0.3289519174630686,\n",
       " -0.3289161828602897,\n",
       " -0.3288807442353911,\n",
       " -0.3288455980270223,\n",
       " -0.32881074073156147,\n",
       " -0.3287761689019197,\n",
       " -0.32874187914634795,\n",
       " -0.32870786812732133,\n",
       " -0.3286741325604057,\n",
       " -0.32864066921318175,\n",
       " -0.32860747490417924,\n",
       " -0.328574546501859,\n",
       " -0.32854188092359105,\n",
       " -0.32850947513469536,\n",
       " -0.32847732614745917,\n",
       " -0.32844543102023027,\n",
       " -0.3284137868564895,\n",
       " -0.3283823908039762,\n",
       " -0.32835124005381083,\n",
       " -0.32832033183966053,\n",
       " -0.3282896634369054,\n",
       " -0.32825923216184166,\n",
       " -0.32822903537089204,\n",
       " -0.328199070459838,\n",
       " -0.3281693348630812,\n",
       " -0.32813982605288905,\n",
       " -0.32811054153871305,\n",
       " -0.3280814788664647,\n",
       " -0.3280526356178422,\n",
       " -0.3280240094096762,\n",
       " -0.3279955978932593,\n",
       " -0.3279673987537324,\n",
       " -0.32793940970944485,\n",
       " -0.32791162851136474,\n",
       " -0.3278840529424715]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
