{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('cleaned_train_data.csv')\n",
    "test_data = pd.read_csv('cleaned_test_data.csv')\n",
    "X_train = train_data.drop(['income'], axis=1, inplace=False)\n",
    "y_train = train_data['income']\n",
    "X_test = test_data.drop(['income'], axis=1, inplace=False)\n",
    "y_test = test_data['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_test):\n",
    "    # Todo: encode categorical features using OneHotEncoder class\n",
    "    # Todo: standardize numerical features using StandardScaler()class\n",
    "    # Todo: return processed training and test sets\n",
    "    train_processed = pd.DataFrame(data = {\"age\": StandardScaler().fit_transform(X_train.iloc[:, 0:1]).T[0]})\n",
    "    test_processed = pd.DataFrame(data = {\"age\": StandardScaler().fit_transform(X_test.iloc[:, 0:1]).T[0]})\n",
    "    col = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "    ohe = OneHotEncoder()\n",
    "    for i in col:\n",
    "        train_col = np.unique(X_train[i].values)\n",
    "        test_col = np.unique(X_test[i].values)\n",
    "        train = pd.DataFrame(data=ohe.fit_transform(X_train[i].values.reshape(-1, 1)).toarray(), columns=train_col)\n",
    "        test = pd.DataFrame(data=ohe.fit_transform(X_test[i].values.reshape(-1, 1)).toarray(), columns=test_col)\n",
    "        \n",
    "        for f in train_col:\n",
    "            title = i + \".\" + f\n",
    "            train_processed[title] = train[f]\n",
    "            \n",
    "        for t in test_col:\n",
    "            title = i + \".\" + t\n",
    "            test_processed[title] = test[t]\n",
    "            \n",
    "    stnd_col = [\"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "    stnd_train = StandardScaler().fit_transform(X_train.iloc[:, 8:11]).T \n",
    "    stnd_test = StandardScaler().fit_transform(X_test.iloc[:, 8:11]).T\n",
    "    for z in range(3):\n",
    "        title = \"standardized.\" + stnd_col[z]\n",
    "        train_processed[title] = stnd_train[z]\n",
    "        test_processed[title] = stnd_test[z]\n",
    "    \n",
    "    # might have to add columns of zeros for test data that doesnt contain all outcomes as train\n",
    "    # columns would be {'native.country.Holand-Netherlands'(col 63),'workclass.Never-worked'(col 3),'workclass.Without-pay'(col 8)}\n",
    "    test_processed.insert(3, \"workclass.Never-worked\", np.zeros((len(test_processed))))\n",
    "    test_processed.insert(8, \"workclass.Without-pay\", np.zeros((len(test_processed))))\n",
    "    test_processed.insert(63, \"native.country.Holand-Netherlands\", np.zeros((len(test_processed))))\n",
    "    return train_processed, test_processed\n",
    "\n",
    "train_processed, test_processed = preprocess_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.6899456 ,  0.        , ..., -0.2325727 ,\n",
       "        -0.21558026, -0.03580076],\n",
       "       [ 1.        ,  1.1296067 ,  0.        , ..., -0.2325727 ,\n",
       "         3.94933449,  0.77598017],\n",
       "       [ 1.        ,  0.32356136,  0.        , ..., -0.2325727 ,\n",
       "        -0.21558026, -0.03580076],\n",
       "       ...,\n",
       "       [ 1.        ,  0.39683821,  1.        , ..., -0.2325727 ,\n",
       "        -0.21558026,  1.18187063],\n",
       "       [ 1.        , -1.14197563,  0.        , ..., -0.2325727 ,\n",
       "        -0.21558026, -0.03580076],\n",
       "       [ 1.        , -0.55576084,  0.        , ..., -0.2325727 ,\n",
       "        -0.21558026, -0.03580076]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_data.drop(['income'], axis=1, inplace=False)\n",
    "X = np.hstack((np.ones((X_train.shape[0], 1)), train_processed))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_data['income']\n",
    "y = y_train.values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(samples, w):\n",
    "        # Todo: Compute and return the predicted probability of each sample having an income >= 50K\n",
    "        z = np.dot(samples, w)\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(samples, threshold=0.5):\n",
    "        return predict_prob(samples) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, X, w):\n",
    "        # Todo: compute and return gradient of the average log likelihood on the training set\n",
    "        # y is a vector\n",
    "        # X is feature matrix\n",
    "        #predict_prob(X, w) is the probability of every row given the weights and a row of data\n",
    "        diff = y - predict_prob(X, w)\n",
    "        gradient = np.dot(X.T, diff)\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def gradient_ascent(X, y, lr, its):\n",
    "        # Todo: Run gradient ascent to learn model weights\n",
    "        # Todo: Store the average log likelihood and the prediction accuracy \n",
    "        #       on the training and test sets after every gradient ascent iteration\n",
    "        \n",
    "        # need y vector and X array(featuresxexamples)\n",
    "        w = np.zeros((X.shape[1]))\n",
    "        \n",
    "        for i in range(its):\n",
    "            gradient = compute_gradient(y, X, w)\n",
    "            w = w + (lr * gradient)\n",
    "            y_hat = predict_prob(X, w)\n",
    "            \n",
    "            ll = (1/len(y_hat)) * sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))\n",
    "            \n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (94,) (94,26049) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-adf8511832fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgradient_ascent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-167-75817b6f3ba0>\u001b[0m in \u001b[0;36mgradient_ascent\u001b[0;34m(X, y, lr, its)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (94,) (94,26049) "
     ]
    }
   ],
   "source": [
    "gradient_ascent(X, y, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100, Log-Likelihood: -18055.790906406015\n",
      "Iteration 2/100, Log-Likelihood: nan\n",
      "Iteration 3/100, Log-Likelihood: nan\n",
      "Iteration 4/100, Log-Likelihood: nan\n",
      "Iteration 5/100, Log-Likelihood: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-163-2ac4bf08f213>:21: RuntimeWarning: divide by zero encountered in log\n",
      "  log_likelihood = np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
      "<ipython-input-163-2ac4bf08f213>:21: RuntimeWarning: invalid value encountered in multiply\n",
      "  log_likelihood = np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6/100, Log-Likelihood: nan\n",
      "Iteration 7/100, Log-Likelihood: nan\n",
      "Iteration 8/100, Log-Likelihood: nan\n",
      "Iteration 9/100, Log-Likelihood: nan\n",
      "Iteration 10/100, Log-Likelihood: nan\n",
      "Iteration 11/100, Log-Likelihood: nan\n",
      "Iteration 12/100, Log-Likelihood: nan\n",
      "Iteration 13/100, Log-Likelihood: nan\n",
      "Iteration 14/100, Log-Likelihood: nan\n",
      "Iteration 15/100, Log-Likelihood: nan\n",
      "Iteration 16/100, Log-Likelihood: nan\n",
      "Iteration 17/100, Log-Likelihood: nan\n",
      "Iteration 18/100, Log-Likelihood: nan\n",
      "Iteration 19/100, Log-Likelihood: nan\n",
      "Iteration 20/100, Log-Likelihood: nan\n",
      "Iteration 21/100, Log-Likelihood: nan\n",
      "Iteration 22/100, Log-Likelihood: nan\n",
      "Iteration 23/100, Log-Likelihood: nan\n",
      "Iteration 24/100, Log-Likelihood: nan\n",
      "Iteration 25/100, Log-Likelihood: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-163-2ac4bf08f213>:4: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26/100, Log-Likelihood: nan\n",
      "Iteration 27/100, Log-Likelihood: nan\n",
      "Iteration 28/100, Log-Likelihood: nan\n",
      "Iteration 29/100, Log-Likelihood: nan\n",
      "Iteration 30/100, Log-Likelihood: nan\n",
      "Iteration 31/100, Log-Likelihood: nan\n",
      "Iteration 32/100, Log-Likelihood: nan\n",
      "Iteration 33/100, Log-Likelihood: nan\n",
      "Iteration 34/100, Log-Likelihood: nan\n",
      "Iteration 35/100, Log-Likelihood: nan\n",
      "Iteration 36/100, Log-Likelihood: nan\n",
      "Iteration 37/100, Log-Likelihood: nan\n",
      "Iteration 38/100, Log-Likelihood: nan\n",
      "Iteration 39/100, Log-Likelihood: nan\n",
      "Iteration 40/100, Log-Likelihood: nan\n",
      "Iteration 41/100, Log-Likelihood: nan\n",
      "Iteration 42/100, Log-Likelihood: nan\n",
      "Iteration 43/100, Log-Likelihood: nan\n",
      "Iteration 44/100, Log-Likelihood: nan\n",
      "Iteration 45/100, Log-Likelihood: nan\n",
      "Iteration 46/100, Log-Likelihood: nan\n",
      "Iteration 47/100, Log-Likelihood: nan\n",
      "Iteration 48/100, Log-Likelihood: nan\n",
      "Iteration 49/100, Log-Likelihood: nan\n",
      "Iteration 50/100, Log-Likelihood: nan\n",
      "Iteration 51/100, Log-Likelihood: nan\n",
      "Iteration 52/100, Log-Likelihood: nan\n",
      "Iteration 53/100, Log-Likelihood: nan\n",
      "Iteration 54/100, Log-Likelihood: nan\n",
      "Iteration 55/100, Log-Likelihood: nan\n",
      "Iteration 56/100, Log-Likelihood: nan\n",
      "Iteration 57/100, Log-Likelihood: nan\n",
      "Iteration 58/100, Log-Likelihood: nan\n",
      "Iteration 59/100, Log-Likelihood: nan\n",
      "Iteration 60/100, Log-Likelihood: nan\n",
      "Iteration 61/100, Log-Likelihood: nan\n",
      "Iteration 62/100, Log-Likelihood: nan\n",
      "Iteration 63/100, Log-Likelihood: nan\n",
      "Iteration 64/100, Log-Likelihood: nan\n",
      "Iteration 65/100, Log-Likelihood: nan\n",
      "Iteration 66/100, Log-Likelihood: nan\n",
      "Iteration 67/100, Log-Likelihood: nan\n",
      "Iteration 68/100, Log-Likelihood: nan\n",
      "Iteration 69/100, Log-Likelihood: nan\n",
      "Iteration 70/100, Log-Likelihood: nan\n",
      "Iteration 71/100, Log-Likelihood: nan\n",
      "Iteration 72/100, Log-Likelihood: nan\n",
      "Iteration 73/100, Log-Likelihood: nan\n",
      "Iteration 74/100, Log-Likelihood: nan\n",
      "Iteration 75/100, Log-Likelihood: nan\n",
      "Iteration 76/100, Log-Likelihood: nan\n",
      "Iteration 77/100, Log-Likelihood: nan\n",
      "Iteration 78/100, Log-Likelihood: nan\n",
      "Iteration 79/100, Log-Likelihood: nan\n",
      "Iteration 80/100, Log-Likelihood: nan\n",
      "Iteration 81/100, Log-Likelihood: nan\n",
      "Iteration 82/100, Log-Likelihood: nan\n",
      "Iteration 83/100, Log-Likelihood: nan\n",
      "Iteration 84/100, Log-Likelihood: nan\n",
      "Iteration 85/100, Log-Likelihood: nan\n",
      "Iteration 86/100, Log-Likelihood: nan\n",
      "Iteration 87/100, Log-Likelihood: nan\n",
      "Iteration 88/100, Log-Likelihood: nan\n",
      "Iteration 89/100, Log-Likelihood: nan\n",
      "Iteration 90/100, Log-Likelihood: nan\n",
      "Iteration 91/100, Log-Likelihood: nan\n",
      "Iteration 92/100, Log-Likelihood: nan\n",
      "Iteration 93/100, Log-Likelihood: nan\n",
      "Iteration 94/100, Log-Likelihood: nan\n",
      "Iteration 95/100, Log-Likelihood: nan\n",
      "Iteration 96/100, Log-Likelihood: nan\n",
      "Iteration 97/100, Log-Likelihood: nan\n",
      "Iteration 98/100, Log-Likelihood: nan\n",
      "Iteration 99/100, Log-Likelihood: nan\n",
      "Iteration 100/100, Log-Likelihood: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-6.71171783e+01],\n",
       "       [ 4.06820189e+00],\n",
       "       [ 2.40885453e+01],\n",
       "       [-1.52647584e+01],\n",
       "       [-4.24810835e-01],\n",
       "       [-3.49286168e+01],\n",
       "       [ 2.20816694e+01],\n",
       "       [-4.74258704e+01],\n",
       "       [-1.27324119e+01],\n",
       "       [-2.51092454e+00],\n",
       "       [-9.14671155e+00],\n",
       "       [-2.01656450e+01],\n",
       "       [ 4.10900578e+01],\n",
       "       [ 5.79691332e+01],\n",
       "       [-9.56491505e+01],\n",
       "       [ 6.46990603e+01],\n",
       "       [ 7.28893785e+01],\n",
       "       [-1.19905523e+02],\n",
       "       [-5.88977778e+01],\n",
       "       [-2.52492525e+01],\n",
       "       [ 2.42230891e+00],\n",
       "       [ 4.66001865e+01],\n",
       "       [-6.79993633e+00],\n",
       "       [-5.93432054e+01],\n",
       "       [-1.26481004e+01],\n",
       "       [-1.20991791e+01],\n",
       "       [-6.17261189e+00],\n",
       "       [-2.30246529e-01],\n",
       "       [-2.63610802e+01],\n",
       "       [ 8.35475733e+01],\n",
       "       [-6.60397993e+01],\n",
       "       [-3.86321776e+01],\n",
       "       [-3.28467286e+01],\n",
       "       [-4.33186545e+01],\n",
       "       [-5.24774082e+00],\n",
       "       [ 3.33527845e+01],\n",
       "       [ 1.18034776e+01],\n",
       "       [ 1.86859341e+01],\n",
       "       [ 3.58422454e+01],\n",
       "       [-3.15001536e+01],\n",
       "       [-7.77314749e+00],\n",
       "       [-3.33750833e+01],\n",
       "       [-1.52446646e+01],\n",
       "       [-4.16163764e+01],\n",
       "       [-2.88386792e+01],\n",
       "       [ 5.97307727e+01],\n",
       "       [-2.44052657e+01],\n",
       "       [-4.27119126e+01],\n",
       "       [-3.72974555e+01],\n",
       "       [-2.98197228e+01],\n",
       "       [ 1.19262805e+00],\n",
       "       [ 1.15100282e+00],\n",
       "       [-2.58946660e+00],\n",
       "       [-4.08321938e+00],\n",
       "       [ 1.96085166e+00],\n",
       "       [-2.38527086e+00],\n",
       "       [-8.47159011e-02],\n",
       "       [-3.59815006e+00],\n",
       "       [ 1.61182302e+00],\n",
       "       [ 9.50620099e-01],\n",
       "       [ 4.56627895e+00],\n",
       "       [-3.29032316e+00],\n",
       "       [-9.73491092e-02],\n",
       "       [ 5.42329896e-02],\n",
       "       [-1.52562000e-01],\n",
       "       [-3.81389819e-02],\n",
       "       [-5.15492482e-01],\n",
       "       [-6.17652721e-01],\n",
       "       [-3.48266416e+00],\n",
       "       [ 1.23812436e+00],\n",
       "       [ 1.73760777e+00],\n",
       "       [ 4.80415191e+00],\n",
       "       [-7.46753789e-01],\n",
       "       [ 2.50800917e+00],\n",
       "       [-6.20656628e-01],\n",
       "       [-1.81391269e+01],\n",
       "       [-2.19803667e+00],\n",
       "       [-9.78359622e-01],\n",
       "       [-1.36637013e+00],\n",
       "       [ 7.76457372e+00],\n",
       "       [-2.23560362e+00],\n",
       "       [-2.01884647e+00],\n",
       "       [-3.71390429e+00],\n",
       "       [ 8.50689869e-02],\n",
       "       [-6.73047757e+00],\n",
       "       [ 1.22740133e+00],\n",
       "       [-6.22541953e-01],\n",
       "       [-1.14142670e+00],\n",
       "       [-3.17206102e+01],\n",
       "       [-5.05479042e+00],\n",
       "       [ 2.52957310e-01],\n",
       "       [ 7.01683138e+01],\n",
       "       [ 2.35618924e+01],\n",
       "       [ 1.94510540e+01]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_regression(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    m, n = X.shape  \n",
    "\n",
    "    weights = np.zeros((n, 1))  # Initialize weights to zeros\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        scores = np.dot(X, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        errors = y - predictions\n",
    "\n",
    "        gradient = np.dot(X.T, errors)\n",
    "        weights += learning_rate * gradient\n",
    "\n",
    "        # Calculate log-likelihood\n",
    "        log_likelihood = np.sum(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{num_iterations}, Log-Likelihood: {log_likelihood}\")\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Example usage:\n",
    "# X_train is your feature matrix, where each row is a sample and each column is a feature\n",
    "# y_train is the corresponding binary label (0 or 1)\n",
    "# Make sure X_train and y_train are NumPy arrays\n",
    "y_train = train_data['income']\n",
    "y = y_train.values\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_train = train_data.drop(['income'], axis=1, inplace=False)\n",
    "X = np.hstack((np.ones((X_train.shape[0], 1)), train_processed))\n",
    "X\n",
    "\n",
    "# Train logistic regression model\n",
    "logistic_regression(X, y, learning_rate=0.01, num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
